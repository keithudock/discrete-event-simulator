# discrete-event-simulator
* The program I have created is a discrete event simulator in C++.The simulation has four queues. The CPU, Disk1, and Disk2 queue are FIFO queues that takes the first job in the queue without regard to the time it arrived. The event queue is a priority queue that orders events in chronological order (from lowest to highest time). 
* The file config.txt contains  all the configurations needed to begin the simulation. Configurations included are seed, init_time, fin_time, arrive_min, arrive_max, quit_prob, cpu_min, cpu_max, disk1_min, disk1_max, disk2_min, disk2_max. 
* The seed is used to generate random numbers. To generate random numbers, I used a random device and uniform distributor to create random numbers uniformly spread across an interval. The init_time and fin_time are the start and end times of the simulation. All min/max configurations are intervals that the random number generator will use to generate a random time that an event will occur (i.e. an event in the CPU will be busy for some time on the interval cpu_min to cpu_max). There is a Configuration class that stores all of these configurations as well as the function to read the configurations from config.txt. 
* There is also a CPU and Disk class that act as each component in the simulation. Each has a status that is 0 if idle and 1 if busy. Each  also contains a job that is stored in the component, if the component is busy. The Job class creates a Job object that has a job number, time at which the job occurs, the status of the job, and the type of job. A CPU, and two Disks (Disk1 and Disk2) are created in the main class, as well as random number generators for each min/max interval defined in the config file. I then create the 4 queues for CPU (FIFO), Disk1 and Disk2 (FIFO), and Events (Priority). Queues are created from the <queue> header file, as well as all standard queue functions. 
* I then define variables for average and max queue size, average and max response time, throughput, and utilization. The start condition is pushed onto the Events queue, and then the end condition is pushed onto the Events queue. I then open our log file (log.txt) and begin our simulation. The first thing the simulation does is write the Job information at the top of the event queue to the log.
*The conditions are handled by if statements rather than switch cases. Either could have been used, I just decided to use if else statements. If the event at the top of the Events queue was of type CPU, the simulation then determines the status of that job. If the job is arriving, then it creates a new job to be put in the event queue after some arrival time. The current job is then sent to the CPU queue. If the CPU queue is idle, then that job is put into the CPU to be processed. The response time is calculated and then that event is put into the CPU queue. If the CPU is busy, then nothing happens and the simulation moves forward. If the event is finishing, it is randomly decided if that job exits the simulation, or gets moved onto a disk. If it gets moved onto a disk, the simulation puts that event in the disk with the smallest queue, or a random disk queue if they are both the same size. 
*The Disks follow a similar routine. If an event arrives at the disk, add it to the disk queue. If the disk is idle, put the job into the disk, calculate the time it spends in the disk, and created an event for when the job will finish in the disk. If the disk is busy, do nothing and continue with the simulation. If a job finishes at the disk, send it back to the CPU.
This will run until the simulations reaches the final time, and then I break the simulation loop. When the simulation is over, close the file.
*Then print the results for average and max queue size, average and max response time, throughput, and utilization to the screen, and I put a reminder in to open log.txt to see the simulation. I tested the program by changing the min/max intervals and the seed of the configuration. 
